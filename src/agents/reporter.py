"""Reporter Agent - formats and delivers code review results.

This agent:
- Combines findings from Analyzer and Context agents
- Formats results as GitHub-compatible markdown
- Prioritizes issues by severity
- Creates actionable recommendations
"""

from typing import Dict, List, Optional
from dataclasses import dataclass

from tools.security_scanner import SecurityScanResult
from tools.complexity_analyzer import CodeComplexityResult
from tools.dependency_analyzer import ImpactAnalysis


@dataclass
class ReviewReport:
    """Complete code review report."""
    
    summary: str
    security_section: str
    complexity_section: str
    context_section: str
    ai_recommendations: str
    overall_verdict: str  # "APPROVE", "COMMENT", "REQUEST_CHANGES"
    

class ReporterAgent:
    """Agent for formatting and delivering code review reports."""
    
    def generate_report(
        self,
        analyzer_results: Dict,
        context_results: Dict,
        pr_metadata: Optional[Dict] = None
    ) -> ReviewReport:
        """Generate comprehensive code review report.
        
        Args:
            analyzer_results: Results from Analyzer Agent
            context_results: Results from Context Agent
            pr_metadata: Optional PR metadata (title, author, etc.)
            
        Returns:
            ReviewReport with formatted sections
        """
        # Generate sections
        summary = self._generate_summary(analyzer_results, context_results, pr_metadata)
        security_section = self._format_security_findings(analyzer_results.get("security_issues", {}))
        complexity_section = self._format_complexity_findings(analyzer_results.get("complexity_analysis", {}))
        context_section = self._format_context_findings(context_results.get("impact_analysis"))
        ai_recommendations = self._combine_ai_insights(
            analyzer_results.get("ai_recommendations", ""),
            context_results.get("ai_insights", "")
        )
        
        # Determine verdict
        verdict = self._determine_verdict(analyzer_results, context_results)
        
        return ReviewReport(
            summary=summary,
            security_section=security_section,
            complexity_section=complexity_section,
            context_section=context_section,
            ai_recommendations=ai_recommendations,
            overall_verdict=verdict
        )
    
    def format_as_github_comment(self, report: ReviewReport) -> str:
        """Format report as GitHub comment markdown.
        
        Args:
            report: ReviewReport to format
            
        Returns:
            GitHub-compatible markdown string
        """
        lines = [
            "## AI Code Review Report",
            "",
            report.summary,
            "",
            "---",
            ""
        ]
        
        # Security section
        if report.security_section:
            lines.extend([
                "### Security Analysis",
                "",
                report.security_section,
                ""
            ])
        
        # Complexity section
        if report.complexity_section:
            lines.extend([
                "### Complexity Analysis",
                "",
                report.complexity_section,
                ""
            ])
        
        # Context section
        if report.context_section:
            lines.extend([
                "### Impact & Dependencies",
                "",
                report.context_section,
                ""
            ])
        
        # AI recommendations
        if report.ai_recommendations:
            lines.extend([
                "---",
                "",
                "### AI Recommendations",
                "",
                report.ai_recommendations,
                ""
            ])
        
        # Verdict
        lines.extend([
            "---",
            "",
            f"**Overall Assessment:** {self._format_verdict(report.overall_verdict)}",
            "",
            "<sub>Generated by AI Code Review System | Powered by Google ADK & Gemini</sub>"
        ])
        
        return "\n".join(lines)
    
    def _generate_summary(
        self,
        analyzer_results: Dict,
        context_results: Dict,
        pr_metadata: Optional[Dict]
    ) -> str:
        """Generate executive summary."""
        diff_analysis = analyzer_results.get("diff_analysis", {})
        impact = context_results.get("impact_analysis")
        
        # Count findings
        total_security = sum(
            result.total_issues 
            for result in analyzer_results.get("security_issues", {}).values()
        )
        total_complexity = sum(
            result.high_complexity_count
            for result in analyzer_results.get("complexity_analysis", {}).values()
        )
        
        lines = [
            f"**Changes:** {diff_analysis.get('files_changed', 0)} files modified, "
            f"+{diff_analysis.get('total_additions', 0)} / -{diff_analysis.get('total_deletions', 0)} lines",
            ""
        ]
        
        if total_security > 0:
            lines.append(f"- **Security:** {total_security} potential issues found")
        else:
            lines.append("- **Security:** No issues detected")
        
        if total_complexity > 0:
            lines.append(f"- **Complexity:** {total_complexity} high-complexity functions")
        else:
            lines.append("- **Complexity:** Within acceptable limits")
        
        if impact:
            lines.append(f"- **Impact:** Risk level {impact.risk_level.upper()}")
            if impact.breaking_changes:
                lines.append(f"- **Breaking Changes:** {len(impact.breaking_changes)} detected")
        
        return "\n".join(lines)
    
    def _format_security_findings(self, security_issues: Dict) -> str:
        """Format security findings."""
        if not security_issues:
            return "No security issues detected."
        
        lines = []
        for file_path, result in security_issues.items():
            if result.total_issues == 0:
                continue
                
            lines.append(f"**{file_path}**")
            lines.append(f"- {result.total_issues} issues: "
                        f"High: {result.high_severity_count}, "
                        f"Medium: {result.medium_severity_count}, "
                        f"Low: {result.low_severity_count}")
            
            # Show top 3 critical issues
            critical_issues = [i for i in result.issues if i.issue_severity == "HIGH"][:3]
            if not critical_issues:
                critical_issues = result.issues[:3]
            
            for issue in critical_issues:
                lines.append(f"  - Line {issue.line_number}: {issue.issue_text[:80]}")
            
            lines.append("")
        
        return "\n".join(lines)
    
    def _format_complexity_findings(self, complexity_analysis: Dict) -> str:
        """Format complexity findings."""
        if not complexity_analysis:
            return "No complexity issues detected."
        
        lines = []
        for file_path, result in complexity_analysis.items():
            if result.high_complexity_count == 0:
                continue
                
            lines.append(f"**{file_path}**")
            lines.append(f"- {result.high_complexity_count} high-complexity functions "
                        f"(avg complexity: {result.average_complexity:.1f})")
            
            # Show top 3 most complex
            complex_funcs = [f for f in result.functions if f.complexity_rank in ('D', 'E', 'F')][:3]
            for func in complex_funcs:
                lines.append(f"  - `{func.name}`: CC={func.cyclomatic_complexity} (Rank {func.complexity_rank})")
            
            lines.append("")
        
        return "\n".join(lines)
    
    def _format_context_findings(self, impact: Optional[ImpactAnalysis]) -> str:
        """Format context and dependency findings."""
        if not impact:
            return "No context analysis available."
        
        lines = [
            f"**Risk Level:** {impact.risk_level.upper()}",
            f"**Affected Modules:** {len(impact.affected_modules)}"
        ]
        
        if impact.breaking_changes:
            lines.append(f"**Breaking Changes:** {len(impact.breaking_changes)}")
            for change in impact.breaking_changes[:3]:
                lines.append(f"- {change}")
            if len(impact.breaking_changes) > 3:
                lines.append(f"- ... and {len(impact.breaking_changes) - 3} more")
        
        return "\n".join(lines)
    
    def _combine_ai_insights(
        self,
        analyzer_recommendations: str,
        context_insights: str
    ) -> str:
        """Combine AI insights from both agents."""
        sections = []
        
        if analyzer_recommendations:
            sections.append("#### Code Quality Insights\n" + analyzer_recommendations)
        
        if context_insights:
            sections.append("#### Context & Integration Insights\n" + context_insights)
        
        return "\n\n".join(sections) if sections else "No AI insights available."
    
    def _determine_verdict(
        self,
        analyzer_results: Dict,
        context_results: Dict
    ) -> str:
        """Determine overall review verdict.
        
        Returns:
            "APPROVE", "COMMENT", or "REQUEST_CHANGES"
        """
        # Check for critical issues
        total_high_security = sum(
            result.high_severity_count
            for result in analyzer_results.get("security_issues", {}).values()
        )
        
        impact = context_results.get("impact_analysis")
        has_breaking_changes = impact and len(impact.breaking_changes) > 0
        high_risk = impact and impact.risk_level == "high"
        
        # Decision logic
        if total_high_security > 0 or has_breaking_changes:
            return "REQUEST_CHANGES"
        
        total_medium_security = sum(
            result.medium_severity_count
            for result in analyzer_results.get("security_issues", {}).values()
        )
        
        if total_medium_security > 0 or high_risk:
            return "COMMENT"
        
        return "APPROVE"
    
    def _format_verdict(self, verdict: str) -> str:
        """Format verdict with emoji and description."""
        verdicts = {
            "APPROVE": "âœ… APPROVE - Changes look good, no blocking issues",
            "COMMENT": "ðŸ’¬ COMMENT - Review suggestions provided, please address",
            "REQUEST_CHANGES": "ðŸš« REQUEST CHANGES - Critical issues must be fixed"
        }
        return verdicts.get(verdict, verdict)
