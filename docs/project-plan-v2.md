# AI Code Review System v2 - Project Plan

**Project Name:** GitHub-First PR Review System  
**Tagline:** "Interactive Code Review Bot that Learns Your Team's Standards"  
**Track:** Enterprise Agents  
**Target Score:** 95-100 points  
**Timeline:** Nov 20 â†’ Dec 1, 2025 (11 days remaining)

---

## ğŸ¯ Strategic Positioning

### What Changed from v1:
**v1 (abandoned):** Generic diff analysis + over-engineered Memory Bank  
**v2 (this):** Deep GitHub integration + learning from PR feedback

### Market Context (Nov 2025):
- **GitHub Copilot** generates code at AI speed â†’ bottleneck is review
- **PR-based workflow** is industry standard (986M code pushes on GitHub)
- **Team standards vary** - generic linters create noise, not value
- **Interactive learning** - bots should adapt, not just report

### Our Unique Value:
"Code review bot that posts inline comments on PR like a human, learns your team's standards from your feedback, and stops nagging about things you don't care about."

### Key Differentiators:
1. **Inline Comments** - Not a single report, but contextual threads on each line
2. **Interactive Learning** - Analyzes developer responses, builds team standards
3. **Incremental Reviews** - Tracks PR progress: "Fixed 3, 2 remain"
4. **GitHub-Native** - Uses PR as storage, not duplicating data

---

## ğŸ—ï¸ Architecture Overview

### Core Principle: GitHub = Our Database

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GitHub (Storage & UI)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PR Entity (First-Class Citizen) â”‚   â”‚
â”‚  â”‚  â”œâ”€ Commits (history)            â”‚   â”‚
â”‚  â”‚  â”œâ”€ Review Comments (threads)    â”‚   â”‚
â”‚  â”‚  â”œâ”€ Files Changed (diff)         â”‚   â”‚
â”‚  â”‚  â””â”€ Metadata (author, labels)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Webhook
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     GitHub Integration Layer            â”‚
â”‚  - Webhook handler                      â”‚
â”‚  - PR context loader                    â”‚
â”‚  - Comment formatter/poster             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Core Analysis Agents              â”‚
â”‚  - Analyzer: security, complexity       â”‚
â”‚  - Context: dependencies, impact        â”‚
â”‚  - Learner: extract standards from      â”‚
â”‚              developer feedback         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Memory Bank (Minimal)                â”‚
â”‚  - Team standards ONLY                  â”‚
â”‚  - Evidence: links to PR threads        â”‚
â”‚  - No raw data duplication              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Components:

**1. GitHub Integration Layer**
- **Webhook Handler:** Receive PR events (opened, synchronize, comment)
- **PR Context Loader:** 
  - Load PR diff, files, commits
  - Load previous reviews from THIS PR
  - Load open/resolved threads
- **Comment Manager:**
  - Create inline review comments (specific line)
  - Reply in threads
  - Resolve threads when fixed

**2. Core Agents**

**Analyzer Agent:**
- Role: Find issues in code
- Input: Diff + file content
- Tools: Bandit (security), Radon (complexity), custom rules
- Output: List of issues with location (file, line)

**Context Agent:**
- Role: Understand broader impact
- Input: Changed files + full repo
- Tools: AST parser, dependency analyzer
- Output: Affected modules, integration risks

**Learner Agent:**
- Role: Extract team standards from feedback
- Input: Developer responses in PR threads
- Tools: LLM to parse intent, pattern detection
- Output: Team standards (when sufficient evidence)

**3. Memory Bank (Minimal - Vertex AI RAG)**

**Technology:** Vertex AI RAG Engine (Corpus + File API)

**Stores ONLY:**
```python
# Document in RAG Corpus
{
  "standard_id": "uuid",
  "rule": "PascalCase acceptable for API handlers",
  "category": "naming",
  "scope": "api/handlers/*.py",
  "evidence": [
    {"pr": 123, "thread_id": "comment_456", "developer": "@alice"},
    {"pr": 145, "thread_id": "comment_789", "developer": "@bob"}
  ],
  "confidence": 0.85,  # based on evidence count
  "override": {"naming-convention": "info"}  # downgrade severity
}
```

**Storage:**
- âœ… Vertex AI RAG Corpus (permanent, semantic search)
- âœ… Service Account auth (no API keys)
- âœ… Metadata filters (category, confidence, scope)

**Does NOT store:**
- PR history (GitHub has it)
- Review comments (GitHub has it)
- Diff content (GitHub has it)

---

## ğŸ”„ Workflows

### Workflow 1: First Review (PR Opened)

```
1. Webhook: pull_request.opened
   â””â”€ payload: pr_number, repo, base_sha, head_sha

2. Load Context:
   â”œâ”€ Get PR diff from GitHub
   â”œâ”€ Check: First review? (no previous bot comments)
   â””â”€ Load team standards from Memory Bank

3. Analyze:
   â”œâ”€ Analyzer: Run security + complexity scans
   â”œâ”€ Context: Check dependencies, integration
   â””â”€ Filter issues using team standards

4. Post Review:
   â”œâ”€ For each issue:
   â”‚   â””â”€ Create inline comment on specific line
   â””â”€ Create review summary comment

Result: PR has 10 inline threads + 1 summary
```

### Workflow 2: Incremental Review (New Commit)

```
1. Webhook: pull_request.synchronize
   â””â”€ new commit pushed to PR

2. Load Incremental Context:
   â”œâ”€ Get previous bot review
   â”œâ”€ Load open threads (not fixed yet)
   â”œâ”€ Load resolved threads (what was fixed)
   â””â”€ Get new commits since last review

3. Analyze Changes:
   â”œâ”€ Run analysis on NEW code only
   â””â”€ Check if open issues still present

4. Update Review:
   â”œâ”€ Resolved issues: Add comment "âœ… Fixed in commit abc"
   â”œâ”€ Still open: Update if code changed
   â””â”€ New issues: Create new threads

Result: Progress tracked, no duplicate comments
```

### Workflow 3: Learning from Feedback

```
1. Webhook: pull_request_review_comment.created
   â””â”€ developer replied in thread

2. Analyze Response:
   â”œâ”€ LLM parses developer intent:
   â”‚   â”œâ”€ "Fixed" â†’ acknowledge
   â”‚   â”œâ”€ "This is our standard" â†’ potential learning
   â”‚   â””â”€ "False positive" â†’ adjust confidence
   â””â”€ Store feedback in evidence buffer

3. Check Pattern:
   â”œâ”€ Query: Similar feedback across PRs?
   â””â”€ If 3+ developers say same thing:
       â””â”€ Extract team standard

4. Create Standard:
   â”œâ”€ Store in Memory Bank with evidence
   â”œâ”€ Reply in thread: "âœ… Understood, added to team standards"
   â””â”€ Resolve thread

Result: Bot learns, stops nagging about team conventions
```

---

## ğŸ“‹ Course Concepts Coverage (6 concepts)

### Required: 3 | Delivering: 6

1. âœ… **Multi-Agent System**
   - Analyzer + Context + Learner agents
   - Orchestrated by Integration Layer

2. âœ… **Custom Tools**
   - GitHub API integration (webhooks, comments, threads)
   - Static analyzers (Bandit, Radon)
   - Diff parser

3. âœ… **Memory System**
   - Minimal Memory Bank for team standards
   - Learning from PR thread feedback
   - Evidence-based confidence scoring

4. âœ… **Evaluation Framework**
   - LLM-as-judge for review quality
   - Acceptance rate tracking
   - False positive metrics

5. âœ… **Observability**
   - ADK logging for each agent
   - Webhook event tracking
   - Learning event tracing

6. âœ… **Production Deployment**
   - Vertex AI Agent Engine
   - Cloud Run for webhook handler
   - Service Account auth

---

## ğŸ› ï¸ Technical Stack

### Core:
- **Framework:** Google ADK (Python 3.12)
- **LLM:** Gemini 2.5 Flash via **Vertex AI only** (no dual-mode complexity)
- **Memory:** Vertex AI RAG Engine (for team standards)
- **GitHub:** PyGithub (API client)
- **Tools:** Bandit, Radon, GitPython

### Deployment:
- **Platform:** Vertex AI only (development & production)
- **Runtime:** Vertex AI Agent Engine
- **Webhook:** Cloud Run (receive GitHub events)
- **Auth:** Service Account (no API keys)
- **Secrets:** Secret Manager (GitHub token)

### Why Vertex AI Only:
- âŒ **No dual-mode** (Google AI Studio vs Vertex AI)
- âŒ **No free tier limitations** (quota issues, feature gaps)
- âœ… **Single codebase** (no environment switching)
- âœ… **Production-ready from day 1** (same env for dev & prod)
- âœ… **Cost acceptable** (few dollars for competition worth it)

### Testing:
- **Unit:** pytest with mocks
- **Integration:** Real GitHub API (test repo)
- **E2E:** Trigger webhook â†’ verify comments

---

## ğŸ“¦ What We Reuse from v1

### Copy As-Is (Stable):

```
capstone/                       capstone2/
â”œâ”€â”€ tools/                  â†’   â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ diff_parser.py          â”‚   â”œâ”€â”€ diff_parser.py
â”‚   â”œâ”€â”€ security_scanner.py     â”‚   â”œâ”€â”€ security_scanner.py
â”‚   â”œâ”€â”€ complexity_analyzer.py  â”‚   â”œâ”€â”€ complexity_analyzer.py
â”‚   â””â”€â”€ repo_merger.py          â”‚   â””â”€â”€ repo_merger.py
â”œâ”€â”€ tests/fixtures/         â†’   â”œâ”€â”€ tests/fixtures/
â”‚   â”œâ”€â”€ test-app/               â”‚   â”œâ”€â”€ test-app/
â”‚   â””â”€â”€ diffs/                  â”‚   â””â”€â”€ diffs/
â”œâ”€â”€ pyproject.toml          â†’   â”œâ”€â”€ pyproject.toml (adapted)
â”œâ”€â”€ requirements/           â†’   â”œâ”€â”€ requirements/
â””â”€â”€ docs/                   â†’   â””â”€â”€ docs/
    â”œâ”€â”€ capstone-requirements   â”‚   â”œâ”€â”€ capstone-requirements
    â””â”€â”€ market-trends-2025      â”‚   â””â”€â”€ market-trends-2025
```

### Redesign (New Architecture):

- âŒ `src/agents/*` â†’ NEW: GitHub-aware agents
- âŒ `src/models.py` â†’ NEW: PR-centric models
- âŒ `src/memory/*` â†’ NEW: Minimal Memory Bank
- âŒ `src/github/` â†’ NEW: Integration layer
- âŒ Tests â†’ NEW: GitHub API mocks

---

## ğŸ¬ Demo Strategy (3 PRs)

### Test Repository Setup:
```
Create: RostislavDublin/review-bot-demo
- Simple Python app
- 3 PRs pre-created with different scenarios
```

### PR #1: "Generic Bot Behavior"
```
Code: PEP8 violations, but intentional (team uses own style)

Bot Initial Review:
â”œâ”€ 15 issues flagged (all style/naming)
â””â”€ Generic PEP8 enforcement

Developer Response:
â”œâ”€ Thread 1: "We use PascalCase for handlers, not snake_case"
â”œâ”€ Thread 2: "120 char limit, not 80"
â””â”€ Thread 3: "We allow wildcard imports in __init__.py"

Result: Bot stores feedback, doesn't learn yet (need more evidence)
```

### PR #2: "Building Evidence"
```
Code: Similar style "violations"

Bot Review:
â”œâ”€ Flags same issues again
â””â”€ But notes: "Similar feedback in PR#1"

Developer Response:
â”œâ”€ @alice: "Already told you, PascalCase is fine"
â”œâ”€ @bob: "Yeah, we always use PascalCase for handlers"

Bot Learns:
â”œâ”€ 3 developers confirmed pattern
â”œâ”€ Creates team standard
â””â”€ Replies: "âœ… Got it! Added to team standards"

Result: Standard created with evidence from 2 PRs
```

### PR #3: "Adapted Behavior"
```
Code: Same style + 2 real security issues

Bot Review:
â”œâ”€ â„¹ï¸ "PascalCase detected (matches team standard)"
â”œâ”€ â„¹ï¸ "Line 95 chars (team allows 120)"
â”œâ”€ âŒ "SQL injection on line 42"
â””â”€ âŒ "Hardcoded secret on line 67"

Developer:
â”œâ”€ Fixes 2 security issues
â””â”€ No noise about style

Result: Bot provides value, respects team context
```

---

## ğŸ“Š Evaluation Metrics

### Quality Metrics:
1. **Precision:** % of flagged issues that are real problems
2. **Recall:** % of real problems that were flagged
3. **Acceptance Rate:** % of suggestions that developers implement
4. **False Positive Rate:** % of flagged issues marked as "not an issue"

### Learning Metrics:
1. **Standards Learned:** Count of team standards extracted
2. **Learning Speed:** PRs needed to learn a standard (target: 2-3)
3. **Confidence Accuracy:** Do high-confidence standards get accepted?

### User Experience:
1. **Review Latency:** Time from commit to bot comment (target: <2 min)
2. **Noise Reduction:** % decrease in irrelevant issues (PR1 vs PR3)
3. **Thread Engagement:** % of bot comments that get developer replies

---

## ğŸ“… Implementation Timeline (11 days)

### Days 3-4 (Nov 20-21): Foundation
- [x] Architecture design
- [x] Clean v2 branch
- [ ] GitHub integration layer (webhooks, API client)
- [ ] Basic PR context loader

### Days 5-6 (Nov 22-23): Core Agents
- [ ] Analyzer agent (reuse tools from v1)
- [ ] Context agent (dependency analysis)
- [ ] Inline comment formatter

### Days 7-8 (Nov 24-25): Learning System
- [ ] Learner agent (parse feedback)
- [ ] Minimal Memory Bank (team standards)
- [ ] Evidence collection & pattern detection

### Days 9-10 (Nov 26-27): Integration & Testing
- [ ] E2E workflow (webhook â†’ review â†’ feedback â†’ learn)
- [ ] Test repo with 3 PRs
- [ ] Integration tests with GitHub API

### Days 11-12 (Nov 28-29): Deployment & Evaluation
- [ ] Deploy to Vertex AI
- [ ] Evaluation framework (LLM-as-judge)
- [ ] Metrics collection

### Day 13 (Nov 30): Documentation & Submission
- [ ] README with demo
- [ ] Video recording (3 min)
- [ ] Submit to Kaggle

---

## ğŸš§ Risks & Mitigation

### Risk 1: GitHub API Rate Limits
- **Mitigation:** Use authenticated requests (5000/hour)
- **Fallback:** Cache PR data locally during testing

### Risk 2: Learning Takes Too Long
- **Mitigation:** Lower threshold to 2 confirmations (not 3)
- **Fallback:** Pre-seed 1-2 standards for demo

### Risk 3: Webhook Latency
- **Mitigation:** Use Cloud Run (fast cold start)
- **Fallback:** Async processing, acknowledge webhook immediately

### Risk 4: Inline Comments Complexity
- **Mitigation:** Start with simple format, iterate
- **Fallback:** Use review-level comments if inline fails

---

## âœ… Success Criteria

### Minimum (Pass):
- âœ… Demonstrates 3+ course concepts
- âœ… Working webhook integration
- âœ… Posts review comments on PR
- âœ… Clear documentation

### Target (95-100 points):
- âœ… 6 course concepts demonstrated
- âœ… Interactive learning from feedback
- âœ… 3-PR demo showing evolution
- âœ… Production deployment working
- âœ… Evaluation framework with metrics
- âœ… Professional documentation + video

---

**Last Updated:** Nov 20, 2025  
**Status:** Day 3 - Architecture complete, starting implementation  
**Branch:** v2-github-first
