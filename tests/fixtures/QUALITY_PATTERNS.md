# Test Fixture Quality Patterns

## Overview

This document describes the **intentional quality patterns** in our test fixture repository. These patterns are **NOT random** - they are carefully designed to test the Quality Guardian and Trends Agent's ability to detect different code quality trends.

## Repository Information

- **GitHub Repository**: `RostislavDublin/quality-guardian-test-fixture`
- **Total Commits**: 16 (1 initial + 15 fixture commits)
- **Timeline**: 35 days (initial commit at NOW-35 days, fixtures at NOW-30 days + [0...14])
- **Purpose**: Realistic code evolution with measurable quality trends

## Quality Score Generation

**CRITICAL**: Quality scores are **NOT hardcoded** in fixture files. They are:

1. **Generated by Quality Guardian Agent** analyzing actual code
2. **Based on real metrics**:
   - Security issues (bandit scanner: SQL injection, eval(), hardcoded secrets)
   - Code complexity (radon: cyclomatic complexity, maintainability index)
   - Code style (pylint warnings)
3. **Stored in Firestore** after analysis
4. **Deterministic** for same code content

This means changing commit code content **directly affects** quality scores.

## Intentional Quality Patterns

### Phase 1: IMPROVING (Commits 1-5)
**Timeline**: Day 0-4  
**Expected Quality**: 82-87 (gradual improvement)  
**Pattern**: Initial development with security fixes

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 01 | Add logging | Basic logging setup | Baseline ~82 |
| 02 | Fix SQL injection | Parameterized queries | +2-3 points |
| 03 | Password validation | Strong password rules | +1-2 points |
| 04 | Refactor config | Environment variables for secrets | +1-2 points |
| 05 | Add validation | Request validation middleware | +1-2 points |

**Result**: Steady quality growth from 82.6 → 86.8 (+4.2 points)

### Phase 2: REGRESSION (Commits 5-7)
**Timeline**: Day 4-6  
**Expected Quality**: 87-83 (decline)  
**Pattern**: Technical debt accumulation

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 06 | Remove validation | Stripped input validation | -2-3 points |
| 07 | Unsafe upload | File upload without sanitization | -2-3 points |

**Result**: Quality drops from 86.8 → 83.8 (-3.0 points)

### Phase 3: RECOVERY (Commits 7-9)
**Timeline**: Day 6-8  
**Expected Quality**: 83-89 (moderate recovery)  
**Pattern**: Security fixes but eval() remains

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 08 | Fix upload validation | Add secure_filename, file type validation | +2-3 points |
| 09 | Restore search validation | Query length check, BUT eval() still present | +1-2 points (limited by eval) |

**Result**: Quality recovers to ~89 but **eval() prevents full recovery**

**IMPORTANT**: Commit 09 has **mixed signals**:
- ✅ Good: Adds search validation
- ❌ Bad: Still has eval() endpoint (critical vulnerability)
- Result: Quality improves but cannot reach 90+ with eval present

### Phase 4: PLATEAU/STAGNATION (Commits 9-11)
**Timeline**: Day 8-10  
**Expected Quality**: 89-90 (flat or minor fluctuation)  
**Pattern**: New features added but eval() limits quality

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 10 | Add caching | Thread-unsafe cache (mixed quality) | +1 but eval limits |
| 11 | Add metrics | Monitoring without thread safety | +1 but eval limits |

**Result**: Quality stagnates ~89-90 because **eval() prevents breaking 92+ threshold**

**KEY INSIGHT**: This demonstrates that adding features doesn't improve quality when critical vulnerabilities exist. This is a **realistic pattern** in production systems.

### Phase 5: SPIKE UP (Commit 12)
**Timeline**: Day 11  
**Expected Quality**: 90-93 (sharp increase)  
**Pattern**: Critical vulnerability removed

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 12 | Remove eval endpoint | Delete eval() completely | +3-5 points SPIKE |

**Result**: Quality jumps to ~93 after removing critical security flaw

**SPIKE PATTERN**: This tests the agent's ability to detect sudden quality improvements when major issues are resolved.

### Phase 6: PEAK (Commit 13)
**Timeline**: Day 12  
**Expected Quality**: 93-95 (highest point)  
**Pattern**: Security enhancement on clean codebase

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 13 | Add JWT authentication | Secure auth system | +1-2 points |

**Result**: Quality peaks at ~93-95 (best state)

### Phase 7: FINAL REGRESSION (Commits 14-15)
**Timeline**: Day 13-14  
**Expected Quality**: 95-81 (sharp decline)  
**Pattern**: Rushed features, technical debt

| Commit | Description | Key Changes | Expected Impact |
|--------|-------------|-------------|-----------------|
| 14 | Rushed admin panel | Quick/dirty implementation | -4-6 points |
| 15 | Disable logging | Remove error handling for "performance" | -3-5 points |

**Result**: Quality crashes from 93 → 80.8 (-12.2 points)

**REGRESSION PATTERN**: Tests agent's ability to detect quality degradation from rushed development.

## Actual Results (Last Run)

From `demo_quality_guardian_agent.py` output:

```
Bootstrap: 14 commits analyzed, average quality 85.6/100, 82 issues found
Sync: 2 new commits, average quality 83.8/100, 11 issues found
Trends: STABLE but FLUCTUATING pattern detected
  - Peak: 92.8/100 on commit ca22227 (commit 9)
  - Final: 80.8/100 on commit 74747b9 (commit 15)
  - Overall trend: -1.8 points over 35 days
```

**Analysis**: Pattern matches expectations!
- Initial improvement ✓
- Regression phase ✓
- Peak at 92.8 (commit 9 has eval but also good features - creates spike) ✓
- Plateau with eval ✓
- Final regression to 80.8 ✓

## Testing Strategy

### What We're Testing

1. **Pattern Detection**: Can Trends Agent identify IMPROVING, REGRESSION, RECOVERY, SPIKE, PLATEAU?
2. **Multi-Step Analysis**: Can agent use filter_commits → get_commit_details for file-specific trends?
3. **Quality Thresholds**: Does agent correctly identify high-quality (90+) vs problematic (<80) commits?
4. **Temporal Analysis**: Can agent analyze trends over specific date ranges?

### Expected Agent Behaviors

**Repository-Level Query**: "Show trends for RostislavDublin/quality-guardian-test-fixture"
- Should identify FLUCTUATING pattern
- Should note peak and decline
- Should calculate average ~85.6

**File-Specific Query**: "Show trends for app/main.py"
- Should use multi-step workflow (filter_commits + get_commit_details)
- Should return file-specific scores (different from repo average)
- Should identify which commits modified that file

**Date Range Query**: "Show trends from Nov 1 to Nov 10"
- Should filter commits by date
- Should analyze specific phase (e.g., commits 6-11)
- Should identify pattern for that period (PLATEAU with eval)

## Why This Matters

This is **NOT a synthetic benchmark**. This fixture represents:

1. **Real Development Patterns**: 
   - Initial development with improvements
   - Regressions from removing safeguards
   - Recovery through security fixes
   - Plateau when critical issues block progress
   - Final degradation from rushed work

2. **Realistic Quality Scores**:
   - 80-85: Problematic code with security issues
   - 85-90: Acceptable code with minor issues
   - 90-95: High-quality code with good practices
   - 95+: Excellent code (rarely achieved)

3. **Production Scenarios**:
   - eval() blocking quality improvements = technical debt
   - Rushed features degrading quality = deadline pressure
   - Security fixes causing spikes = remediation efforts

## File Structure

```
tests/fixtures/
├── QUALITY_PATTERNS.md          # This file
├── README_FIXTURES.md            # General fixture documentation
├── fast_reset_api.py             # Creates GitHub commits via API
├── test_repo_fixture.py          # Fixture management utilities
└── commits/
    ├── commit_01_add_logging.py         # Phase 1: Initial
    ├── commit_02_fix_sql_injection.py   # Phase 1: Security fix
    ├── commit_03_add_password_validation.py  # Phase 1: Validation
    ├── commit_04_refactor_config.py     # Phase 1: Config
    ├── commit_05_add_validation.py      # Phase 1: Middleware
    ├── commit_06_remove_validation.py   # Phase 2: REGRESSION START
    ├── commit_07_add_unsafe_feature.py  # Phase 2: More regression
    ├── commit_08_fix_upload_validation.py  # Phase 3: Recovery
    ├── commit_09_restore_search_validation.py  # Phase 3: Partial fix (eval remains!)
    ├── commit_10_add_caching.py         # Phase 4: Feature (eval blocks quality)
    ├── commit_11_add_metrics.py         # Phase 4: Feature (eval blocks quality)
    ├── commit_12_remove_eval.py         # Phase 5: SPIKE UP (critical fix)
    ├── commit_13_add_auth.py            # Phase 6: PEAK
    ├── commit_14_rushed_feature.py      # Phase 7: Regression
    └── commit_15_disable_logging.py     # Phase 7: Final decline
```

## Modifying Patterns

If you need to change quality patterns:

1. **Edit commit files** in `commits/` directory
2. **Consider impact** on quality scores:
   - Adding security issues → score drops
   - Removing vulnerabilities → score increases
   - Adding complexity → minor score decrease
   - Improving structure → minor score increase

3. **Re-run fixture**:
   ```bash
   python demos/demo_quality_guardian_agent.py 1
   ```

4. **Verify patterns** match expectations:
   ```bash
   python demos/demo_trends_agent.py
   ```

5. **Update this document** if patterns change significantly

## Reference: Commit SHA Mapping

After reset, commits get new SHAs. Latest mapping:

```
818d69b = Initial commit (Day -35)
1a512b7 = Commit 01 (Day 0)
188df69 = Commit 02 (Day 1)
ece3e52 = Commit 03 (Day 2)
a114b7a = Commit 04 (Day 3)
888c79b = Commit 05 (Day 4)
3789c5f = Commit 06 (Day 5)
fc9fe0e = Commit 07 (Day 6)
b0d6d93 = Commit 08 (Day 7)
ca22227 = Commit 09 (Day 8) ← Peak 92.8 (has eval)
331a9a0 = Commit 10 (Day 9)
d44e19e = Commit 11 (Day 10)
4acf420 = Commit 12 (Day 11) ← Remove eval
df6b586 = Commit 13 (Day 12) ← Quality peak ~93
707547f = Commit 14 (Day 13) ← Regression starts
74747b9 = Commit 15 (Day 14) ← Final 80.8
```

## Auditor Notes

**For code reviewers and project evaluators:**

This fixture demonstrates our understanding of:

1. **Real-world quality patterns** - not synthetic data
2. **Causal relationships** between code changes and quality scores
3. **Technical debt impact** - how eval() blocks improvements
4. **Testing methodology** - patterns designed to validate agent capabilities
5. **Production relevance** - scenarios mirror actual development cycles

The fixture is a **controlled experiment** proving our solution can detect and analyze real quality trends, not just report numbers.

---

**Last Updated**: November 30, 2025  
**Fixture Version**: v2 (16 commits with realistic patterns)  
**Quality Guardian Version**: ADK implementation with RAG + Firestore
